name: qwen3-embedding

services:
  qwen3-embedding:
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    container_name: qwen3-embedding
    ports:
      - "${QWEN3_EMBEDDING_PORT:-7800}:8000"
    volumes:
      - ./models:/models
      - ./cache:/root/.cache
    environment:
      - MODEL_NAME=${QWEN3_EMBEDDING_MODEL:-Qwen/Qwen3-Embedding-4B}
      - HOST=0.0.0.0
      - PORT=8000
      - GPU_MEMORY_UTILIZATION=${QWEN3_EMBEDDING_GPU_MEM:-0.95}
      - MAX_MODEL_LEN=${QWEN3_EMBEDDING_MAX_LEN:-8192}
      - EMBEDDING_DIMENSIONS=${QWEN3_EMBEDDING_DIMENSIONS:-2560}
      - QUANTIZATION=${QWEN3_QUANTIZATION:-fp8}
      - FP8_FORMAT=${QWEN3_FP8_FORMAT:-e4m3}
      - ENABLE_PREFIX_CACHING=${QWEN3_ENABLE_PREFIX_CACHING:-true}
      - TENSOR_PARALLEL_SIZE=1
      - PIPELINE_PARALLEL_SIZE=1
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - CUDA_VISIBLE_DEVICES=0
    mem_limit: ${QWEN3_EMBEDDING_MEMORY_LIMIT:-4g}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      --model ${QWEN3_EMBEDDING_MODEL:-Qwen/Qwen3-Embedding-4B}
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization ${QWEN3_EMBEDDING_GPU_MEM:-0.95}
      --max-model-len ${QWEN3_EMBEDDING_MAX_LEN:-8192}
      --tensor-parallel-size 1
      --quantization ${QWEN3_QUANTIZATION:-fp8}
      --kv-cache-dtype fp8_e4m3
      --enable-prefix-caching
      --max-parallel-loading-workers 4
      --max-num-batched-tokens 65536
      --max-num-seqs 512
      --enforce-eager
      --disable-log-stats
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "environment=${ENVIRONMENT:-dev}"
      - "service_type=ai-service"
      - "service_name=qwen3-embedding"
      - "ai_model_type=embedding"
      - "ai_model_size=4B"
      - "ai_framework=vllm"
      - "container_group=ai-services"
      - "monitoring_enabled=true"
