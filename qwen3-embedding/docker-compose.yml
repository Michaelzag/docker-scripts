name: qwen3-embedding

services:
  qwen3-embedding:
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    container_name: qwen3-embedding
    ports:
      - "${QWEN3_EMBEDDING_PORT:-7800}:8000"
    volumes:
      - ./models:/models
      - ./cache:/root/.cache
    environment:
       - MODEL_NAME=${QWEN3_EMBEDDING_MODEL:-Qwen/Qwen3-Embedding-4B}
       - HOST=0.0.0.0
       - PORT=8000
       - GPU_MEMORY_UTILIZATION=${QWEN3_EMBEDDING_GPU_MEM:-0.5}
       - MAX_MODEL_LEN=${QWEN3_EMBEDDING_MAX_LEN:-8192}
       - ENABLE_PREFIX_CACHING=${QWEN3_ENABLE_PREFIX_CACHING:-true}
       - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    mem_limit: ${QWEN3_EMBEDDING_MEMORY_LIMIT:-8g}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      --model ${QWEN3_EMBEDDING_MODEL:-Qwen/Qwen3-Embedding-4B}
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization ${QWEN3_EMBEDDING_GPU_MEM:-0.5}
      --max-model-len ${QWEN3_EMBEDDING_MAX_LEN:-8192}
      --tensor-parallel-size 1
      --enable-prefix-caching
      --max-parallel-loading-workers 4
      --task embed
      --quantization fp8
      --max-num-batched-tokens ${QWEN3_MAX_NUM_BATCHED_TOKENS:-32768}
      --max-num-seqs ${QWEN3_MAX_NUM_SEQS:-256}
      --max-model-len 8192
      --compilation-config '{"cudagraph_capture_sizes": [512, 256, 128, 64, 32, 16, 8, 4, 2, 1]}'
      --disable-log-stats
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "environment=${ENVIRONMENT:-dev}"
      - "service_type=ai-service"
      - "service_name=qwen3-embedding"
      - "ai_model_type=embedding"
      - "ai_model_size=4B"
      - "ai_framework=vllm"
      - "container_group=ai-services"
      - "monitoring_enabled=true"
